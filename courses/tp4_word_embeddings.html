<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0073)https://cservan.github.io/website/tp3_POS_tagging.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
	<title>Demo 4: Deep Learning for NLP - Word Embeddings</title>
<link href="chrome-extension://lnkdbjbjpnpjeciipoaflmpcddinpjjp/mytube.css" rel="stylesheet" type="text/css"><script src="chrome-extension://lnkdbjbjpnpjeciipoaflmpcddinpjjp/mutationObserver.js"></script><script src="chrome-extension://lnkdbjbjpnpjeciipoaflmpcddinpjjp/mytube.js"></script></head>
<body lang="fr-FR" text="#000000" dir="ltr" style="background: transparent">
<h1 align="center">Demo 4: Deep Learning for NLP - Word Embeddings</h1>
<h3 align="center">Christophe Servan 
</h3>
	<hr>
	<h2>Tools</h2>
	<ul>
		<li>FastText: enable you to train and use word embeddings. 
		
		</li><li>Installation:
			<ul>
				<li>Download the installation script <tt><a href="https://cservan.github.io/website/tools/FT_installation.bash">FT_installation.bash</a></tt> then type:<pre>bash FT_installation.bash</pre><b>WARNING: </b> the tool will be installed by default into the "tools" directory located into your home directory. <br> <br>
				</li><li>Once the installation is done, type <pre>source ~/.bashrc</pre>
				</li><li>Test your installation by typing <pre>fasttext --help</pre> the result should print the help of the fasttext command.
			</li></ul>
	</li></ul>
	<h2>Data</h2>
	
	<ul>
		<li>Use the text8 data file (download <a href="https://cservan.github.io/website/tools/text8.zip">here</a>) to train our word embeddings.
		</li><li>Decompress then using unzip:<pre>unzip -x text8.zip </pre> 
	</li></ul>
	<hr>
	<h2>Report
	</h2>
	<p>You should send me your report <u><b>the day before the next session</b></u> using <b>PDF</b> format
	at the address christophe_dot_servan_at_epita_dot_fr (replace
	_dot_ and _at_ by dots and at, respectively) </p><p>
	<em>Remarks:</em>
	</p><ul>
		<li>If you encounter some issues, feel free to send me an email, I'll answer you ASAP;
		</li><li>If you send me your report lately, you will be penalised ;
		</li><li>the report mark is important for the final course mark ;
		</li><li>jupyternotes and others will be ignored. I want you to write a report, not a script.
		</li><li>no report equal to zero  ;
	</li></ul>
	<p></p>
	<hr>
	<h2>Work to do</h2>
	<ol>
		<ol>
			This TP is here to help you manipulate the word embeddings
			<ul>
				<li>Fristly you will train your word embeddings
				</li><li>First task is to load the them
				</li><li>Then find the closest words
				</li><li>In a fourth part, you will process analogy
				</li><li>Finally, Evaluate the embeddings according a well known dataset
			</li></ul>
	</ol>
	<h3>I. Train word embeddings</h3>
	<ol>
		<p>The main idea is to train your first word embeddings model</p>
		
		<p><b>WARNING:</b> Read carefully the documentation available here: <a href="https://github.com/facebookresearch/fastText">https://github.com/facebookresearch/fastText</a></p>
		<ol>
			<p><strong>Exercice 1:</strong> Train two models, with a vector dimension of 50, 5iteration of training and a window size of 15.
			</p><ol>
				<p><strong>Q1:</strong> Train a cbow model, explain your command line (1pt)</p>
				<p><strong>Q2:</strong> Train a skip-gram model, explain your command line (1pt)</p>
				<p><strong>Q3:</strong> Give example of the vector file, describe it (1pt)</p>
				 </ol>
			<p></p>
        </ol>
	</ol>
	<h3>II. Loading word embeddings</h3>
	<ol>
		<p>
        Loading the WE is not difficult in itself. 
        To make computations fast, we store the whole set of embeddings in a numpy array of shape (num words, dimensions).<br>
        We also build a dictionary (vocab) for mapping words to row index in this array, and another (rev vocab) for mapping indexes back to word forms.
		</p> 
        <p></p>
        <pre>
        def load(filename):
            vocab = {}
            rev_vocab = []
            lines = open(filename).readlines()
            vectors = np.zeros((len(lines), len(lines[0].split()) - 1))
            for i, line in enumerate(lines):
                tokens = line.strip().split()
                vocab[tokens[0]] = i
                rev_vocab.append(tokens[0])
                vectors[i] = [float(value) for value in tokens[1:]]
            return vocab, rev_vocab, vectors
</pre>
		<ol>
			<p><strong>Exercice 1:</strong> Loading</p>
			<ol>
				<p><strong>Q1: </strong> write in Python the necessary script to load the WE (1pt)</p>
                <p></p>
				<p><strong>Q2: </strong> what "vocab", "rev_vocab" and "vectors" stand for? (1pt)</p>		 
			</ol>
			<p><strong>Exercice 2:</strong> Compute the cosine Similarity</p>
			<ol>
				<p><strong>Q1: </strong> Explain what is the cosine similarity. (1pt)</p>
                <p></p>
				<p><strong>Q2: </strong> How to compute it in python using numpy? (1pt)</p>		 
                <p></p>
				<p><strong>Q3: </strong> What is the cosine similarity between the vectors representing "dog" and
"cat", and what about "dog" and "dentist"? (1pt)</p>
                <p></p>
				<p><strong>Q4: </strong> What is the closest word to "bank": "river" or "trade"? (1pt)</p>		 
			</ol>
		</ol>
	</ol>
	<h3>III. Closest words</h3>
	<ol>
		<p> Now you have your WE model, you will use a tric to compute all the closest words.<br>
        This can be done once for all, by computing the dot product between he matrix containing all
vectors and the transpose of the target word vector (np.dot(vectors, v.T)).<br>
Then we can use some numpy trick to recover the indices of the n highest scores.
        </p>
        <pre>
        def closest(vectors, vector, n=10):
            scores = np.dot(vectors, vector.T)
            indices = np.argpartition(scores, -n)[-n:]
            indices = indices[np.argsort(scores[indices])]
            output = []
            for i in [int(x) for x in indices]:
                output.append((scores[i], i))
            return reversed(output)
</pre>        
		<ol>
			<p><strong>Exercice 1:</strong> Code the function</p>
			<ol>
				<p><strong>Q1: </strong> What "argpartition" stand for? (1pt)</p>
				<p><strong>Q2: </strong> Add Comments to the code. (1pt)</p>
			</ol>
			<p><strong>Exercice 2:</strong> Analysis </p>
			<ol>
				<p><strong>Q1: </strong> What are the closest words to "apple"? (1pt)</p>
				<p><strong>Q2: </strong> What about other words (close to "apple")? (1pt)</p>
				<p><strong>Q3: </strong> Can you find words which have a strange neighborhood? (1pt)</p>
			</ol>
		</ol>
	</ol>
	<h3>IV. Analogy</h3>
	<ol>
		<p> Word analogies can be exposed by translating a word vector in a direction that
corresponds to a linguistic or semantic relationship between two other words.</br>
So if we have <tt>w1</tt> and <tt>w2</tt> in relation <tt>R(w1,w2)</tt>, we can compute the relation
<tt>r=vec(w2)-vec(W1)</tt>, and then apply this relation to the vector of another
word <tt>vec(w3)</tt>. </br>
The word closest to <tt>vec(w3)+r</tt> should also exhibit the same relation.</br>
Therefore, the idea is to use the closest function to find vectors similar to <tt>vec(w2)-vec(w1)+vec(w3)</tt>.
        </p>
		<ol>
			<p><strong>Exercice 1:</strong> Code the analogy function</p>
			<ol>
				<p><strong>Q1: </strong> Can you use the previous code? How far? (1pt)</p>
			</ol>
			<p><strong>Exercice 2:</strong> Solve the analogies </p>
			<ol>
				<p><strong>Q1: </strong> "paris" is to "france" what "delhi" is to ... ? (1pt)</p>
				<p><strong>Q2: </strong> "gates" - "microsoft" + "apple" = ... ? (1pt)</p>
				<p><strong>Q3: </strong> "king" - "man" + "woman" = ... ? (1pt)</p>
				<p><strong>Q4: </strong> "slow" - "slower" + "fast" = ... ? (1pt)</p>
			</ol>
			<p><strong>Exercice 3:</strong> Bonus </p>
			<ol>
				<p><strong>Q1: </strong> Augment the dimension of the WE to 200 and retrain them (1pt)</p>
				<p><strong>Q2: </strong> What are the impact on the analogies? (1pt)</p>
			</ol>
		</ol>
	</ol>
	<h3>V. Evaluation</h3>
	<ol>
		<p> To evaluate the quality of a set of word embeddings, we will use data from the WS-3531 dataset (<a href="https://github.com/k-kawakami/embedding-evaluation">https://github.com/k-kawakami/embedding-evaluation</a>). </br>
The file EN-WS-353-REL.txt contains on each line a pair of words followed by the average of a similarity rating by human judges.
The higher the rating, the more similar the judges though the words in the pair are. </br>
One way to evaluate the quality of word embeddings is to compute the correlation between the cosine similarity of the pairs and human ratings. </br>
Since cosine similarities and judgements are on a different scale, we will use a rank correlation (how similar two rankings are).</br>
To compute Spearman's &rho; correlation, you can use scipy. 
The <tt>spearmanr</tt> function takes two lists of values as argument and returns two values.
The first one is &rho;, the second one is the significance of the estimation (pval). </br>
Here we are only interested in the first one.
        </p>
        <pre>
from scipy import stats
serie1 = [1.3, 2.2, 3.1, 4.0]
serie2 = [4.5, 1.8, 3.2, 2.7]
print(stats.spearmanr(serie1, serie2)[0])
</pre>        
		<ol>
			<p><strong>Exercice 1:</strong> compute the &rho; correlation between human judgements and the word
embeddings you have created for different window length</p>
			<ol>
				<p><strong>Q1: </strong> What is the best window length which correlate the best? (1pt)</p>
				<p><strong>Q2: </strong> What is the best vector dimension which correlate the best? (1pt)</p>
			</ol>
		</ol>
	</ol>
</ol>
 <br> <br>The End.
 
 

</body></html>
